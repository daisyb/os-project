       	       	     +-------------------------+
		     |		CS 140	       |
		     | PROJECT 4: FILE SYSTEMS |
		     |	   DESIGN DOCUMENT     |
		     +-------------------------+

---- GROUP ----

>> Fill in the names and email addresses of your group members.

Daisy Barbanel    <dbarbanel@uchicago.edu>
Raphael Hallerman <rhallerman@uchicago.edu>

---- PRELIMINARIES ----

>> If you have any preliminary comments on your submission, notes for the
>> TAs, or extra credit, please give them here.

>> Please cite any offline or online sources you consulted while
>> preparing your submission, other than the Pintos documentation, course
>> text, lecture notes, and course staff.

		     INDEXED AND EXTENSIBLE FILES
		     ============================

---- DATA STRUCTURES ----

>> A1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

#define INODE_MAGIC 0x494e4f44
#define DIRECT_CNT 122
#define INDIRECT_CNT 1
#define DBL_INDIRECT_CNT 1
#define SECTOR_CNT (DIRECT_CNT + INDIRECT_CNT + DBL_INDIRECT_CNT)
#define PTRS_PER_SECTOR ((off_t) (BLOCK_SECTOR_SIZE / sizeof (block_sector_t)))
#define INODE_SPAN ((DIRECT_CNT
		     + PTRS_PER_SECTOR * INDIRECT_CNT
		     + PTRS_PER_SECTOR * PTRS_PER_SECTOR * DBL_INDIRECT_CNT)
		     * BLOCK_SECTOR_SIZE)

struct inode_disk {
       ....
       block_sector_t sectors[SECTOR_CNT];      /* Sectors */
       ....
};

struct inode {
       struct lock lock;			/* Protects the inode */
       struct lock deny_write_lock;		/* Protects members below */
       struct condition no_writers_cond;	/* 0: writes ok, >0: deny writes */
       int writer_cnt;				/* Number of writers */
};

struct indirect_block {
       block_sector_t sectors[PTRS_PER_SECTOR];	/* block of direct pointers */
       unsigned magic;				/* to help identify indirect_blocks */
};

>> A2: What is the maximum size of a file supported by your inode
>> structure?  Show your work.

   Our inode_disk structs have 122 direct pointers, and 1 indirect and
   double-indirect pointer each.
   Each direct pointer points to a sector = 512 bytes of data, so in total,
   our direct pointers point to 512 * 122 = 62464 bytes of data.
   The indirect pointer points to PTRS_PER_SECTOR = 128 direct pointers,
   and thereby indirectly points to 128 * 512 = 65536 bytes of data.
   The double-indirect pointer points to 128 indirect pointers, thereby
   pointing to 16384 direct pointers, and thereby pointing to 16384 * 512
   = 8388608 bytes of data.
   In total, then, our inode structure supported files up to size
   8388608 + 65536 + 62464 = 33682432 bytes = 8516608 bytes = 8.51 megabytes.
   
---- SYNCHRONIZATION ----

>> A3: Explain how your code avoids a race if two processes attempt to
>> extend a file at the same time.

  

>> A4: Suppose processes A and B both have file F open, both
>> positioned at end-of-file.  If A reads and B writes F at the same
>> time, A may read all, part, or none of what B writes.  However, A
>> may not read data other than what B writes, e.g. if B writes
>> nonzero data, A is not allowed to see all zeros.  Explain how your
>> code avoids this race.

  

>> A5: Explain how your synchronization design provides "fairness".
>> File access is "fair" if readers cannot indefinitely block writers
>> or vice versa.  That is, many processes reading from a file cannot
>> prevent forever another process from writing the file, and many
>> processes writing to a file cannot prevent another process forever
>> from reading the file.



---- RATIONALE ----

>> A6: Is your inode structure a multilevel index?  If so, why did you
>> choose this particular combination of direct, indirect, and doubly
>> indirect blocks?  If not, why did you choose an alternative inode
>> structure, and what advantages and disadvantages does your
>> structure have, compared to a multilevel index?

   Our inode structure is a multilevel index, as this seemed like the
   best way to accomodate the required maximum of 8MB files. We chose
   this particular combination of direct, indirect, and doubly indirect
   blocks because it was the minimum number of indirect and doubly
   indirect blocks required to fulfill the space goal, while still
   optimizing for small files by having at least one indirect block
   and as many direct blocks as possible.

			    SUBDIRECTORIES
			    ==============

---- DATA STRUCTURES ----

>> B1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.


/* Type of an inode */
enum inode_type {
  FILE_INODE, 			/* Ordinary file */
  DIR_INODE			/* Directory */
};


struct inode_disk {
   ...
  enum inode_type type; // FILE_INODE or DIR_INODE
  ...
};

/* Union type to hold either an open file or open directory. Used in file_descriptor */
union data_union
{
  struct file* file; // an open file
  struct dir* dir; // an open directory
};

struct file_descriptor {
  int handle;           // fd handle
  enum inode_type type; // FILE_INODE or DIR_INODE, so we know what type of fd this is
  union data_union data; // Either an open file or an open directory
  struct list_elem elem; // List elem for a threads list of file_descriptors
};

struct thread
{
  ...
  struct dir* working_dir;          /* The proccess's current working directory */
  ..
}
---- ALGORITHMS ----

>> B2: Describe your code for traversing a user-specified path.  How
>> do traversals of absolute and relative paths differ?

Our code parses the parts of a path between slashes one at time. Each
path part is then used to open the next directory level, if
necessary. Once, there is no more path to parse, we find the file, or
an error occurs, traversal stops.

If the path is absolute we begin our traversal from the root
directory. Otherwise we begin traversing from the the current thread's
working_dir.

---- SYNCHRONIZATION ----

>> B4: How do you prevent races on directory entries?  For example,
>> only one of two simultaneous attempts to remove a single file
>> should succeed, as should only one of two simultaneous attempts to
>> create a file with the same name, and so on.

Each inode has a lock. During directory functions, like dir_remove, the
inode of the directory whose enties are being modified is locked.

>> B5: Does your implementation allow a directory to be removed if it
>> is open by a process or if it is in use as a process's current
>> working directory?  If so, what happens to that process's future
>> file system operations?  If not, how do you prevent it?

No we don't allow an open or in use directory to be removed. We do
this by checking how many inode openers the directory that's going to
be removed has. We also check that it is not the same directory as the
the the thread's working_dir parameter which stores the current working
directory.

---- RATIONALE ----

>> B6: Explain why you chose to represent the current directory of a
>> process the way you did.

We chose to represent the current directory of a process as a struct
dir *, which is kept on struct thread. This way keeping the
working_directory from being removed is practically the same as
keeping any other open directory from being removed. Furthermore, if
you need to access something in your current directory you can just
use the already open dir thats kept on your thread. The only downside
is that you have to prevent any attempts by a thread to close its
current directory.

			     BUFFER CACHE
			     ============

---- DATA STRUCTURES ----

>> C1: Copy here the declaration of each new or changed `struct' or
>> `struct' member, global or static variable, `typedef', or
>> enumeration.  Identify the purpose of each in 25 words or less.

/* An entry in the buffer cache */
struct cache_block {
  block_sector_t sector; // disk sector currently stored on cache_block
  uint8_t data[BLOCK_SECTOR_SIZE]; // data copied from disk
  bool dirty;                     // whether block needs to be written back to the disk on eviction
  bool clock_bit; // use bit, used for clock algorithm
  struct lock block_lock; // used to synchronize accesses to individual cache_blocks
};


#define INVALID_SECTOR ((block_sector_t) -1) // Unused blocks set sector to INVALID_SECTOR
#define CACHE_CNT 64 // size of buffer cache

struct cache_block cache[CACHE_CNT]; // The buffer cache itself
struct lock cache_lock; // lock for scanning the buffer cache

---- ALGORITHMS ----

>> C2: Describe how your cache replacement algorithm chooses a cache
>> block to evict.

   We use the clock algorithm. Each cache block has a dirty bit and a
use bit (which we call clock_bit). On each pass, if the clock_bit
is set, then it is unset. If the clock_bit is already unset then that block
is evicted. This way each cache_block gets a 'second chance', so that
we don't end up evicting a block that is frequently in use.

>> C3: Describe your implementation of write-behind.

   Does not apply.
   
>> C4: Describe your implementation of read-ahead.

   Does not apply.

---- SYNCHRONIZATION ----

>> C5: When one process is actively reading or writing data in a
>> buffer cache block, how are other processes prevented from evicting
>> that block?

   Each cache block has a lock which is locked before eviction and
   when reading or writing to a cache block.

>> C6: During the eviction of a block from the cache, how are other
>> processes prevented from attempting to access the block?

   The cache_block is locked right before eviction. Any other thread
   who might want the block trys to acquire the lock first, and if it
   cannot acquire the lock is not able to use that block.

---- RATIONALE ----

>> C7: Describe a file workload likely to benefit from buffer caching,
>> and workloads likely to benefit from read-ahead and write-behind.

   Buffer caching is beneficial for workloads with significant amounts of temporal or spatial locality. These workloads frequently access either the same blocks multiple times or successive blocks within a short time period, so without buffer caching, they'd need to go to disk for each one of these reads/writes.

For temporally local workloads, buffer caching only requires going to disk once, and the program can then continue to read or modify the data within memory. For spatially local workloads, buffer caching reduces the number of disk accesses required to read memory within a single block.

Write-behind is similar, since workloads with temporal locality can continue to write to the same block repeatedly without the cache writing it back to memory until eviction occurs. Read-ahead, on the other hand, optimizes for workloads with spatial locality. These workloads consistently need to access successive blocks, and read-ahead makes the process of reading from disk more efficient by simply bringing multiple blocks into the cache at a time. This way, as long as it is done asynchronously, the workload won't require nearly as many reads from disk.

			   SURVEY QUESTIONS
			   ================

Answering these questions is optional, but it will help us improve the
course in future quarters.  Feel free to tell us anything you
want--these questions are just to spur your thoughts.  You may also
choose to respond anonymously in the course evaluations at the end of
the quarter.

>> In your opinion, was this assignment, or any one of the three problems
>> in it, too easy or too hard?  Did it take too long or too little time?

>> Did you find that working on a particular part of the assignment gave
>> you greater insight into some aspect of OS design?

>> Is there some particular fact or hint we should give students in
>> future quarters to help them solve the problems?  Conversely, did you
>> find any of our guidance to be misleading?

>> Do you have any suggestions for the TAs to more effectively assist
>> students in future quarters?

>> Any other comments?